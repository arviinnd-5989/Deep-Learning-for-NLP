{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    model = VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    model.summary()\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size = (224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        print('>%s' %name)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os import path\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 20:27:13.255772 139847449843520 module_wrapper.py:136] From /home/arvind/.local/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0709 20:27:13.340297 139847449843520 module_wrapper.py:136] From /home/arvind/.local/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      ">1002674143_1b742ab4b8.jpg\n",
      ">1015584366_dfcec3c85a.jpg\n",
      ">1007320043_627395c3d8.jpg\n",
      ">1009434119_febe49276a.jpg\n",
      ">1003163366_44323f5815.jpg\n",
      ">1012212859_01547e3f17.jpg\n",
      ">1001773457_577c3a7d70.jpg\n",
      ">1015118661_980735411b.jpg\n",
      ">1007129816_e794419615.jpg\n",
      ">1000268201_693b08cb0e.jpg\n",
      "Extracted Features: 10\n"
     ]
    }
   ],
   "source": [
    "directory = 'arvind'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' %len(features))\n",
    "dump(features, open('features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/arvind_doc.txt'\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line)<2:\n",
    "            continue\n",
    "        \n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 10\n"
     ]
    }
   ],
   "source": [
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d' %len (descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "def clean_descriptions(descriptions):\n",
    "    re_punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [re_punc.sub('',w) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 174\n"
     ]
    }
   ],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' %len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line)<1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            \n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/arvind_doc_train.txt'\n",
    "train = load_set(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 8\n"
     ]
    }
   ],
   "source": [
    "print('Dataset: %d' %len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = load_clean_descriptions('descriptions.txt',train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train=8\n"
     ]
    }
   ],
   "source": [
    "print('Descriptions: train=%d'%len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = load_photo_features('features.pkl', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=8\n"
     ]
    }
   ],
   "source": [
    "print('Photos: train=%d' %len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1000268201_693b08cb0e': ['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       "  'startseq girl going into wooden building endseq',\n",
       "  'startseq little girl climbing into wooden playhouse endseq',\n",
       "  'startseq little girl climbing the stairs to her playhouse endseq',\n",
       "  'startseq little girl in pink dress going into wooden cabin endseq'],\n",
       " '1001773457_577c3a7d70': ['startseq black dog and spotted dog are fighting endseq',\n",
       "  'startseq black dog and tricolored dog playing with each other on the road endseq',\n",
       "  'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n",
       "  'startseq two dogs of different breeds looking at each other on the road endseq',\n",
       "  'startseq two dogs on pavement moving toward each other endseq'],\n",
       " '1002674143_1b742ab4b8': ['startseq little girl covered in paint sits in front of painted rainbow with her hands in bowl endseq',\n",
       "  'startseq little girl is sitting in front of large painted rainbow endseq',\n",
       "  'startseq small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it endseq',\n",
       "  'startseq there is girl with pigtails sitting in front of rainbow painting endseq',\n",
       "  'startseq young girl with pigtails painting outside in the grass endseq'],\n",
       " '1003163366_44323f5815': ['startseq man lays on bench while his dog sits by him endseq',\n",
       "  'startseq man lays on the bench to which white dog is also tied endseq',\n",
       "  'startseq man sleeping on bench outside with white and black dog sitting next to him endseq',\n",
       "  'startseq shirtless man lies on park bench with his dog endseq',\n",
       "  'startseq man laying on bench holding leash of dog sitting on ground endseq'],\n",
       " '1007129816_e794419615': ['startseq man in an orange hat starring at something endseq',\n",
       "  'startseq man wears an orange hat and glasses endseq',\n",
       "  'startseq man with gauges and glasses is wearing blitz hat endseq',\n",
       "  'startseq man with glasses is wearing beer can crocheted hat endseq',\n",
       "  'startseq the man with pierced ears is wearing glasses and an orange hat endseq'],\n",
       " '1007320043_627395c3d8': ['startseq child playing on rope net endseq',\n",
       "  'startseq little girl climbing on red roping endseq',\n",
       "  'startseq little girl in pink climbs rope bridge at the park endseq',\n",
       "  'startseq small child grips onto the red ropes at the playground endseq',\n",
       "  'startseq the small child climbs on red ropes on playground endseq'],\n",
       " '1009434119_febe49276a': ['startseq black and white dog is running in grassy garden surrounded by white fence endseq',\n",
       "  'startseq black and white dog is running through the grass endseq',\n",
       "  'startseq boston terrier is running in the grass endseq',\n",
       "  'startseq boston terrier is running on lush green grass in front of white fence endseq',\n",
       "  'startseq dog runs on the green grass near wooden fence endseq'],\n",
       " '1012212859_01547e3f17': ['startseq dog shakes its head near the shore red ball next to it endseq',\n",
       "  'startseq white dog shakes on the edge of beach with an orange ball endseq',\n",
       "  'startseq dog with orange ball at feet stands on shore shaking off water endseq',\n",
       "  'startseq white dog playing with red ball on the shore near the water endseq',\n",
       "  'startseq white dog with brown ears standing near water with head turned to one side endseq']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 146\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' %vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1,len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape = (4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation = 'relu')(fe1)\n",
    "    inputs2 = Input(shape = (max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    decoder1 = add([fe2,se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "    model = Model(inputs=[inputs1,inputs2], outputs = outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png',show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454 samples, validate on 63 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 5.2773 - val_loss: 4.5468\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.54677, saving model to model.h5\n",
      "Epoch 2/20\n",
      " - 1s - loss: 4.3097 - val_loss: 4.3615\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.54677 to 4.36155, saving model to model.h5\n",
      "Epoch 3/20\n",
      " - 1s - loss: 3.9765 - val_loss: 4.3566\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.36155 to 4.35664, saving model to model.h5\n",
      "Epoch 4/20\n",
      " - 1s - loss: 3.6636 - val_loss: 4.2699\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.35664 to 4.26994, saving model to model.h5\n",
      "Epoch 5/20\n",
      " - 1s - loss: 3.5725 - val_loss: 4.3115\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.26994\n",
      "Epoch 6/20\n",
      " - 1s - loss: 3.4108 - val_loss: 4.1985\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.26994 to 4.19851, saving model to model.h5\n",
      "Epoch 7/20\n",
      " - 1s - loss: 3.3286 - val_loss: 4.2719\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.19851\n",
      "Epoch 8/20\n",
      " - 1s - loss: 3.2580 - val_loss: 4.3882\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.19851\n",
      "Epoch 9/20\n",
      " - 1s - loss: 3.1194 - val_loss: 4.3037\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.19851\n",
      "Epoch 10/20\n",
      " - 1s - loss: 3.0834 - val_loss: 4.3880\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.19851\n",
      "Epoch 11/20\n",
      " - 1s - loss: 2.9825 - val_loss: 4.3596\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.19851\n",
      "Epoch 12/20\n",
      " - 1s - loss: 2.9098 - val_loss: 4.4103\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.19851\n",
      "Epoch 13/20\n",
      " - 1s - loss: 2.8255 - val_loss: 4.4244\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.19851\n",
      "Epoch 14/20\n",
      " - 1s - loss: 2.7836 - val_loss: 4.4002\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 4.19851\n",
      "Epoch 15/20\n",
      " - 1s - loss: 2.7175 - val_loss: 4.4416\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4.19851\n",
      "Epoch 16/20\n",
      " - 1s - loss: 2.6260 - val_loss: 4.4109\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 4.19851\n",
      "Epoch 17/20\n",
      " - 1s - loss: 2.5612 - val_loss: 4.4108\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4.19851\n",
      "Epoch 18/20\n",
      " - 1s - loss: 2.4697 - val_loss: 4.4760\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.19851\n",
      "Epoch 19/20\n",
      " - 1s - loss: 2.4883 - val_loss: 4.4356\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 4.19851\n",
      "Epoch 20/20\n",
      " - 1s - loss: 2.3717 - val_loss: 4.3842\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.19851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30617569e8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1train,X2train],ytrain,epochs=20,verbose=2,callbacks=[checkpoint],validation_data = ([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 19\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' %max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/arvind_doc_dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_set(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2\n"
     ]
    }
   ],
   "source": [
    "print('Dataset: %d' %len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descriptions = load_clean_descriptions('descriptions.txt',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: test = 2\n"
     ]
    }
   ],
   "source": [
    "print( 'Descriptions: test = %d' %len(test_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = load_photo_features('features.pkl', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: test = 2\n"
     ]
    }
   ],
   "source": [
    "print('Photos: test = %d' %len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions,test_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 16:33:54.180612 139847449843520 module_wrapper.py:136] From /home/arvind/.local/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 19, 256)      37376       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 19, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 146)          37522       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,714,834\n",
      "Trainable params: 1,714,834\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen = max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = argmax(yhat)\n",
    "        word = word_for_id(yhat,tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_summary(summary):\n",
    "    index = summary.find('startseq')\n",
    "    if index > -1:\n",
    "        summary = summary[len('startseq '):]\n",
    "    index = summary.find(' endseq')\n",
    "    if index > -1:\n",
    "        summary = summary[:index]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        yhat = cleanup_summary(yhat)\n",
    "        references = [cleanup_summary(d).split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0,0,0,0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5,0.5,0,0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3,0.3,0.3,0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.416667\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arvind/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/arvind/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/arvind/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "model = load_model(filename)\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open('tokenizer.pkl','rb'))\n",
    "max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_features(filename):\n",
    "    model = VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    image=load_img(filename, target_size = (224,224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image,verbose=0)\n",
    "    return feature\n",
    "\n",
    "photo = extracted_features('example.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little little little little into into into\n"
     ]
    }
   ],
   "source": [
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "description = cleanup_summary(description)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
