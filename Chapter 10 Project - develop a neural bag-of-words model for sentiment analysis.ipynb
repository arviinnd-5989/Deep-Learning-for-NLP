{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "process_docs('txt_sentoken/pos',vocab)\n",
    "process_docs('txt_sentoken/neg',vocab)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c>=min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines,filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list(tokens,'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews to lines of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "#         if filename.startswith('cv9'):\n",
    "#             continue\n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs,labels = load_clean_dataset(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 1800\n"
     ]
    }
   ],
   "source": [
    "print(len(docs),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab,is_train):\n",
    "    lines =list()\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab,is_train)\n",
    "    docs= neg+pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab)#,True)\n",
    "test_docs,ytest = load_clean_dataset(vocab)#,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs,mode='binary')\n",
    "# print(Xtrain.shape,Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = Xtest.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape = (n_words,),activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "#     model.summary()\n",
    "#     plot_model(model,to_file=\"model.png\",show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.4509 - accuracy: 0.7990\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.0545 - accuracy: 0.9945\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 7.8774e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 6.0480e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f271198c390>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(n_words)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)\n",
    "# loss,acc = model.evaluate(Xtest,ytest,verbose=0)\n",
    "# print(\"Test Accuracy: %f\"%(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate different encoding schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs,mode=mode)\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs,mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        model = define_model(n_words)\n",
    "        model.fit(Xtrain,ytrain,epochs=10,verbose=0)\n",
    "        _ ,acc = model.evaluate(Xtest,ytest,verbose=0)\n",
    "        scores.append(acc)\n",
    "        print(\"%d accuracy: %s\"%((i+1),acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['binary','count','tfidf','freq']\n",
    "results = DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 accuracy: 0.9350000023841858\n",
      "2 accuracy: 0.9150000214576721\n",
      "3 accuracy: 0.9200000166893005\n",
      "4 accuracy: 0.9350000023841858\n",
      "5 accuracy: 0.9200000166893005\n",
      "6 accuracy: 0.9350000023841858\n",
      "7 accuracy: 0.925000011920929\n",
      "8 accuracy: 0.9399999976158142\n",
      "9 accuracy: 0.9300000071525574\n",
      "10 accuracy: 0.9200000166893005\n",
      "1 accuracy: 0.9049999713897705\n",
      "2 accuracy: 0.8949999809265137\n",
      "3 accuracy: 0.8849999904632568\n",
      "4 accuracy: 0.8799999952316284\n",
      "5 accuracy: 0.8999999761581421\n",
      "6 accuracy: 0.8999999761581421\n",
      "7 accuracy: 0.9100000262260437\n",
      "8 accuracy: 0.8949999809265137\n",
      "9 accuracy: 0.8999999761581421\n",
      "10 accuracy: 0.9049999713897705\n",
      "1 accuracy: 0.8949999809265137\n",
      "2 accuracy: 0.8700000047683716\n",
      "3 accuracy: 0.8849999904632568\n",
      "4 accuracy: 0.8700000047683716\n",
      "5 accuracy: 0.8849999904632568\n",
      "6 accuracy: 0.8700000047683716\n",
      "7 accuracy: 0.8899999856948853\n",
      "8 accuracy: 0.8650000095367432\n",
      "9 accuracy: 0.8849999904632568\n",
      "10 accuracy: 0.8799999952316284\n",
      "1 accuracy: 0.8700000047683716\n",
      "2 accuracy: 0.8700000047683716\n",
      "3 accuracy: 0.8500000238418579\n",
      "4 accuracy: 0.8600000143051147\n",
      "5 accuracy: 0.8600000143051147\n",
      "6 accuracy: 0.8650000095367432\n",
      "7 accuracy: 0.8600000143051147\n",
      "8 accuracy: 0.875\n",
      "9 accuracy: 0.8899999856948853\n",
      "10 accuracy: 0.8650000095367432\n",
      "         binary      count      tfidf       freq\n",
      "count  10.00000  10.000000  10.000000  10.000000\n",
      "mean    0.92750   0.897500   0.879500   0.866500\n",
      "std     0.00858   0.009204   0.010124   0.010814\n",
      "min     0.91500   0.880000   0.865000   0.850000\n",
      "25%     0.92000   0.895000   0.870000   0.860000\n",
      "50%     0.92750   0.900000   0.882500   0.865000\n",
      "75%     0.93500   0.903750   0.885000   0.870000\n",
      "max     0.94000   0.910000   0.895000   0.890000\n"
     ]
    }
   ],
   "source": [
    "for mode in modes:\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "\n",
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens = clean_doc(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ''.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line],mode='binary')\n",
    "    yhat = model.predict(encoded,verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a bad movie.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (54.672%)\n"
     ]
    }
   ],
   "source": [
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)'%(text,sentiment,percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
